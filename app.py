import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# é…ç½®
MODEL_PATH = "zoomerwork/model_usingultrachat"

print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
)

# è®¾ç½®pad_tokenï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")

def chat(message, history):
    try:
        # æ„å»ºå¯¹è¯å†å²
        conversation = ""
        
        # å¤„ç†ä¸åŒæ ¼å¼çš„history
        if history:
            # åªä¿ç•™æœ€è¿‘çš„5è½®å¯¹è¯
            recent_history = history[-5:] if len(history) > 5 else history
            
            for item in recent_history:
                # å…¼å®¹ä¸åŒçš„historyæ ¼å¼
                if isinstance(item, (list, tuple)) and len(item) == 2:
                    user_msg, bot_msg = item
                    conversation += f"User: {user_msg}\nAssistant: {bot_msg}\n"
                elif isinstance(item, dict):
                    # æœ‰äº›ç‰ˆæœ¬ä½¿ç”¨å­—å…¸æ ¼å¼
                    user_msg = item.get('user', item.get('role') == 'user' and item.get('content', ''))
                    bot_msg = item.get('assistant', item.get('role') == 'assistant' and item.get('content', ''))
                    if user_msg and bot_msg:
                        conversation += f"User: {user_msg}\nAssistant: {bot_msg}\n"
        
        conversation += f"User: {message}\nAssistant:"
        
        # Tokenize
        inputs = tokenizer(
            conversation, 
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        )
        
        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # è§£ç 
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–assistantçš„å›å¤
        if "Assistant:" in full_response:
            response = full_response.split("Assistant:")[-1].strip()
        else:
            response = full_response.strip()
        
        # æ¸…ç†å¤šä½™å†…å®¹
        if "User:" in response:
            response = response.split("User:")[0].strip()
        
        return response
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        print(f"History æ ¼å¼: {type(history)}")
        if history:
            print(f"History å†…å®¹ç¤ºä¾‹: {history[:2]}")  # æ‰“å°å‰2ä¸ªå…ƒç´ çœ‹æ ¼å¼
        import traceback
        traceback.print_exc()
        return f"æŠ±æ­‰ï¼Œå‘ç”Ÿäº†é”™è¯¯: {str(e)}"

# åˆ›å»ºç•Œé¢
demo = gr.ChatInterface(
    fn=chat,
    title="ğŸ¤– Chatbot",
    description="åŸºäºLLMå¾®è°ƒçš„å¯¹è¯æ¨¡å‹",
    examples=[
        "Hello",
        "Can you help me to write some python code?",
        "Explain what is ML",
    ],
)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
    )