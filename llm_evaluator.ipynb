{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– LLMæ¨¡å‹ä¸‹è½½ä¸è¯„ä¼°å·¥å…·\n",
    "\n",
    "è¿™ä¸ªnotebookå¯ä»¥å¸®ä½ ä»HuggingFaceä¸‹è½½æ¨¡å‹å¹¶è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚\n",
    "\n",
    "**è¯„ä¼°ç»´åº¦ï¼š**\n",
    "1. å›°æƒ‘åº¦ (Perplexity) - 30åˆ†\n",
    "2. æ¨ç†é€Ÿåº¦ (Inference Speed) - 30åˆ†  \n",
    "3. ç”Ÿæˆè´¨é‡ (Generation Quality) - 25åˆ†\n",
    "4. æ¨¡å‹æ•ˆç‡ (Model Efficiency) - 15åˆ†\n",
    "\n",
    "**æ¨èè¿è¡Œç¯å¢ƒï¼š**\n",
    "- Google Colab (å…è´¹T4 GPU)\n",
    "- æœ¬åœ°Jupyter (éœ€è¦GPU)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: å®‰è£…ä¾èµ–åŒ…\n",
    "\n",
    "é¦–æ¬¡è¿è¡Œéœ€è¦å®‰è£…æ‰€éœ€çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…\n",
    "!pip install torch transformers datasets accelerate bitsandbytes pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 2: å¯¼å…¥åº“å’Œå®šä¹‰è¯„ä¼°å™¨ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jovyan/.local/lib/python3.13/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åº“å¯¼å…¥æˆåŠŸï¼\n",
      "PyTorchç‰ˆæœ¬: 2.9.0+cu128\n",
      "CUDAå¯ç”¨: True\n",
      "GPU: NVIDIA H100 80GB HBM3 MIG 1g.20gb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥æˆåŠŸï¼\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLMEvaluatorç±»å®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "class LLMEvaluator:\n",
    "    \"\"\"LLMæ¨¡å‹è¯„ä¼°å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, device: str = \"auto\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = self._setup_device(device)\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def _setup_device(self, device: str) -> str:\n",
    "        if device == \"auto\":\n",
    "            return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return device\n",
    "    \n",
    "    def download_and_load_model(self, load_in_8bit: bool = False, load_in_4bit: bool = False):\n",
    "        \"\"\"ä»HuggingFaceä¸‹è½½å¹¶åŠ è½½æ¨¡å‹\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"æ­£åœ¨ä¸‹è½½æ¨¡å‹: {self.model_name}\")\n",
    "        print(f\"ç›®æ ‡è®¾å¤‡: {self.device}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        try:\n",
    "            # åŠ è½½tokenizer\n",
    "            print(\"ğŸ“¥ åŠ è½½Tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            print(\"âœ… TokenizeråŠ è½½å®Œæˆ\")\n",
    "            \n",
    "            # åŠ è½½æ¨¡å‹\n",
    "            print(\"ğŸ“¥ åŠ è½½æ¨¡å‹...\")\n",
    "            model_kwargs = {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"torch_dtype\": torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            }\n",
    "            \n",
    "            if load_in_8bit:\n",
    "                model_kwargs[\"load_in_8bit\"] = True\n",
    "                print(\"âš™ï¸  ä½¿ç”¨8bité‡åŒ–\")\n",
    "            elif load_in_4bit:\n",
    "                model_kwargs[\"load_in_4bit\"] = True\n",
    "                print(\"âš™ï¸  ä½¿ç”¨4bité‡åŒ–\")\n",
    "            elif self.device == \"cuda\":\n",
    "                model_kwargs[\"device_map\"] = \"auto\"\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "            \n",
    "            if self.device == \"cpu\" and not (load_in_8bit or load_in_4bit):\n",
    "                self.model = self.model.to(self.device)\n",
    "            \n",
    "            print(\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate_perplexity(self, texts: List[str] = None, num_samples: int = 50) -> float:\n",
    "        \"\"\"è¯„ä¼°å›°æƒ‘åº¦\"\"\"\n",
    "        print(\"\\n[1/4] ğŸ“Š è¯„ä¼°å›°æƒ‘åº¦ (Perplexity)...\")\n",
    "        \n",
    "        if texts is None:\n",
    "            try:\n",
    "                print(\"   åŠ è½½WikiText-2æ•°æ®é›†...\")\n",
    "                dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "                texts = [item['text'] for item in dataset if len(item['text']) > 50][:num_samples]\n",
    "            except:\n",
    "                texts = [\n",
    "                    \"The quick brown fox jumps over the lazy dog.\",\n",
    "                    \"Artificial intelligence is transforming the world.\",\n",
    "                    \"Machine learning models require large amounts of data.\"\n",
    "                ]\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, text in enumerate(texts):\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"   å¤„ç†ä¸­: {i+1}/{len(texts)}\")\n",
    "                \n",
    "                encodings = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                input_ids = encodings.input_ids.to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids, labels=input_ids)\n",
    "                total_loss += outputs.loss.item() * input_ids.size(1)\n",
    "                total_tokens += input_ids.size(1)\n",
    "        \n",
    "        perplexity = torch.exp(torch.tensor(total_loss / total_tokens)).item()\n",
    "        \n",
    "        # è¯„åˆ†\n",
    "        if perplexity < 20:\n",
    "            rating = \"ğŸŒŸ ä¼˜ç§€\"\n",
    "        elif perplexity < 50:\n",
    "            rating = \"ğŸ˜Š è‰¯å¥½\"\n",
    "        elif perplexity < 100:\n",
    "            rating = \"ğŸ˜ ä¸­ç­‰\"\n",
    "        else:\n",
    "            rating = \"ğŸ˜ éœ€æ”¹è¿›\"\n",
    "        \n",
    "        print(f\"   âœ… å›°æƒ‘åº¦: {perplexity:.2f} {rating}\")\n",
    "        return perplexity\n",
    "    \n",
    "    def evaluate_generation_quality(self, prompts: List[str] = None) -> Dict:\n",
    "        \"\"\"è¯„ä¼°ç”Ÿæˆè´¨é‡\"\"\"\n",
    "        print(\"\\n[2/4] ğŸ“ è¯„ä¼°ç”Ÿæˆè´¨é‡...\")\n",
    "        \n",
    "        if prompts is None:\n",
    "            prompts = [\n",
    "                \"Explain quantum computing in simple terms:\",\n",
    "                \"Write a short story about a robot:\",\n",
    "                \"What is the capital of France?\",\n",
    "                \"Translate 'Hello' to Spanish:\",\n",
    "                \"List three benefits of exercise:\"\n",
    "            ]\n",
    "        \n",
    "        results = []\n",
    "        generation_times = []\n",
    "        \n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            print(f\"   ç”Ÿæˆä¸­ {i}/{len(prompts)}: {prompt[:40]}...\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            generation_times.append(generation_time)\n",
    "            \n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            results.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": generated_text[len(prompt):].strip(),\n",
    "                \"time\": generation_time\n",
    "            })\n",
    "        \n",
    "        avg_time = np.mean(generation_times)\n",
    "        print(f\"\\n   âœ… å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f}ç§’\")\n",
    "        \n",
    "        return {\n",
    "            \"avg_generation_time\": avg_time,\n",
    "            \"samples\": results\n",
    "        }\n",
    "    \n",
    "    def evaluate_model_size(self) -> Dict:\n",
    "        \"\"\"è¯„ä¼°æ¨¡å‹å¤§å°\"\"\"\n",
    "        print(\"\\n[3/4] ğŸ“ è¯„ä¼°æ¨¡å‹è§„æ¨¡...\")\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        param_size = sum(p.nelement() * p.element_size() for p in self.model.parameters())\n",
    "        buffer_size = sum(b.nelement() * b.element_size() for b in self.model.buffers())\n",
    "        size_mb = (param_size + buffer_size) / 1024**2\n",
    "        \n",
    "        print(f\"   å‚æ•°é‡: {total_params:,} ({total_params/1e9:.2f}B)\")\n",
    "        print(f\"   æ¨¡å‹å¤§å°: {size_mb:.2f} MB ({size_mb/1024:.2f} GB)\")\n",
    "        \n",
    "        return {\n",
    "            \"total_params\": total_params,\n",
    "            \"trainable_params\": trainable_params,\n",
    "            \"size_mb\": size_mb\n",
    "        }\n",
    "    \n",
    "    def evaluate_inference_speed(self, num_runs: int = 10) -> Dict:\n",
    "        \"\"\"è¯„ä¼°æ¨ç†é€Ÿåº¦\"\"\"\n",
    "        print(f\"\\n[4/4] âš¡ è¯„ä¼°æ¨ç†é€Ÿåº¦ (è¿è¡Œ{num_runs}æ¬¡)...\")\n",
    "        \n",
    "        test_input = \"The future of artificial intelligence is\"\n",
    "        inputs = self.tokenizer(test_input, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # é¢„çƒ­\n",
    "        with torch.no_grad():\n",
    "            self.model.generate(**inputs, max_new_tokens=50)\n",
    "        \n",
    "        times = []\n",
    "        tokens_generated = []\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            start = time.time()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            elapsed = time.time() - start\n",
    "            times.append(elapsed)\n",
    "            tokens_generated.append(outputs.shape[1] - inputs.input_ids.shape[1])\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        avg_tokens = np.mean(tokens_generated)\n",
    "        tokens_per_second = avg_tokens / avg_time\n",
    "        \n",
    "        # è¯„çº§\n",
    "        if tokens_per_second > 50:\n",
    "            rating = \"ğŸš€ ä¼˜ç§€\"\n",
    "        elif tokens_per_second > 30:\n",
    "            rating = \"ğŸ˜Š è‰¯å¥½\"\n",
    "        elif tokens_per_second > 10:\n",
    "            rating = \"ğŸ˜ ä¸­ç­‰\"\n",
    "        else:\n",
    "            rating = \"ğŸŒ è¾ƒæ…¢\"\n",
    "        \n",
    "        print(f\"   âœ… æ¨ç†é€Ÿåº¦: {tokens_per_second:.2f} tokens/s {rating}\")\n",
    "        \n",
    "        return {\n",
    "            \"avg_inference_time\": avg_time,\n",
    "            \"tokens_per_second\": tokens_per_second\n",
    "        }\n",
    "    \n",
    "    def _calculate_overall_score(self, results: Dict) -> float:\n",
    "        \"\"\"è®¡ç®—ç»¼åˆè¯„åˆ†\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # å›°æƒ‘åº¦è¯„åˆ† (30åˆ†)\n",
    "        if results.get(\"perplexity\"):\n",
    "            ppl = results[\"perplexity\"]\n",
    "            if ppl < 20:\n",
    "                score += 30\n",
    "            elif ppl < 50:\n",
    "                score += 25\n",
    "            elif ppl < 100:\n",
    "                score += 20\n",
    "            elif ppl < 200:\n",
    "                score += 15\n",
    "            else:\n",
    "                score += 10\n",
    "        \n",
    "        # æ¨ç†é€Ÿåº¦è¯„åˆ† (30åˆ†)\n",
    "        if results.get(\"inference_speed\"):\n",
    "            tps = results[\"inference_speed\"][\"tokens_per_second\"]\n",
    "            if tps > 50:\n",
    "                score += 30\n",
    "            elif tps > 30:\n",
    "                score += 25\n",
    "            elif tps > 20:\n",
    "                score += 20\n",
    "            elif tps > 10:\n",
    "                score += 15\n",
    "            else:\n",
    "                score += 10\n",
    "        \n",
    "        # ç”Ÿæˆè´¨é‡è¯„åˆ† (25åˆ†)\n",
    "        if results.get(\"generation\"):\n",
    "            avg_time = results[\"generation\"][\"avg_generation_time\"]\n",
    "            if avg_time < 2:\n",
    "                score += 25\n",
    "            elif avg_time < 5:\n",
    "                score += 20\n",
    "            elif avg_time < 10:\n",
    "                score += 15\n",
    "            else:\n",
    "                score += 10\n",
    "        \n",
    "        # æ¨¡å‹æ•ˆç‡è¯„åˆ† (15åˆ†)\n",
    "        if results.get(\"model_size\"):\n",
    "            params_b = results[\"model_size\"][\"total_params\"] / 1e9\n",
    "            if 1 <= params_b <= 10:\n",
    "                score += 15\n",
    "            elif params_b < 1:\n",
    "                score += 12\n",
    "            elif params_b <= 20:\n",
    "                score += 10\n",
    "            else:\n",
    "                score += 5\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def run_comprehensive_evaluation(self, show_samples: bool = True) -> Dict:\n",
    "        \"\"\"è¿è¡Œå®Œæ•´è¯„ä¼°\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"ğŸ” å¼€å§‹è¯„ä¼°æ¨¡å‹: {self.model_name}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. å›°æƒ‘åº¦\n",
    "        try:\n",
    "            results[\"perplexity\"] = self.evaluate_perplexity()\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  å›°æƒ‘åº¦è¯„ä¼°å¤±è´¥: {e}\")\n",
    "            results[\"perplexity\"] = None\n",
    "        \n",
    "        # 2. ç”Ÿæˆè´¨é‡\n",
    "        try:\n",
    "            results[\"generation\"] = self.evaluate_generation_quality()\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  ç”Ÿæˆè´¨é‡è¯„ä¼°å¤±è´¥: {e}\")\n",
    "            results[\"generation\"] = None\n",
    "        \n",
    "        # 3. æ¨¡å‹è§„æ¨¡\n",
    "        try:\n",
    "            results[\"model_size\"] = self.evaluate_model_size()\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  æ¨¡å‹è§„æ¨¡è¯„ä¼°å¤±è´¥: {e}\")\n",
    "            results[\"model_size\"] = None\n",
    "        \n",
    "        # 4. æ¨ç†é€Ÿåº¦\n",
    "        try:\n",
    "            results[\"inference_speed\"] = self.evaluate_inference_speed()\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  æ¨ç†é€Ÿåº¦è¯„ä¼°å¤±è´¥: {e}\")\n",
    "            results[\"inference_speed\"] = None\n",
    "        \n",
    "        # ç»¼åˆè¯„åˆ†\n",
    "        score = self._calculate_overall_score(results)\n",
    "        results[\"overall_score\"] = score\n",
    "        \n",
    "        # æ˜¾ç¤ºç»“æœ\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if results.get(\"perplexity\"):\n",
    "            print(f\"å›°æƒ‘åº¦:      {results['perplexity']:.2f}\")\n",
    "        if results.get(\"inference_speed\"):\n",
    "            print(f\"æ¨ç†é€Ÿåº¦:    {results['inference_speed']['tokens_per_second']:.2f} tokens/s\")\n",
    "        if results.get(\"generation\"):\n",
    "            print(f\"ç”Ÿæˆæ—¶é—´:    {results['generation']['avg_generation_time']:.2f}s\")\n",
    "        if results.get(\"model_size\"):\n",
    "            print(f\"å‚æ•°é‡:      {results['model_size']['total_params']/1e9:.2f}B\")\n",
    "        \n",
    "        # è¯„åˆ†ç­‰çº§\n",
    "        if score >= 90:\n",
    "            grade = \"ğŸ† ä¼˜ç§€\"\n",
    "        elif score >= 75:\n",
    "            grade = \"ğŸ¥ˆ è‰¯å¥½\"\n",
    "        elif score >= 60:\n",
    "            grade = \"ğŸ¥‰ ä¸­ç­‰\"\n",
    "        else:\n",
    "            grade = \"ğŸ“ éœ€æ”¹è¿›\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(f\"ç»¼åˆè¯„åˆ†:    {score:.1f}/100 {grade}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # æ˜¾ç¤ºç”Ÿæˆæ ·ä¾‹\n",
    "        if show_samples and results.get(\"generation\") and results[\"generation\"].get(\"samples\"):\n",
    "            print(\"\\nğŸ“ ç”Ÿæˆæ ·ä¾‹:\")\n",
    "            for i, sample in enumerate(results[\"generation\"][\"samples\"][:2], 1):\n",
    "                print(f\"\\n[æ ·ä¾‹ {i}]\")\n",
    "                print(f\"Q: {sample['prompt']}\")\n",
    "                print(f\"A: {sample['response'][:150]}...\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"âœ… LLMEvaluatorç±»å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 3: é…ç½®è¦è¯„ä¼°çš„æ¨¡å‹\n",
    "\n",
    "ä¿®æ”¹ä¸‹é¢çš„ `MODEL_NAME` æ¥è¯„ä¼°ä¸åŒçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é…ç½®å®Œæˆï¼\n",
      "æ¨¡å‹: zoomerwork/model_gemma\n",
      "4bité‡åŒ–: False\n",
      "8bité‡åŒ–: False\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ é…ç½®åŒºåŸŸ - ä¿®æ”¹è¿™é‡Œæ¥è¯„ä¼°ä¸åŒçš„æ¨¡å‹\n",
    "\n",
    "# é€‰æ‹©è¦è¯„ä¼°çš„æ¨¡å‹ï¼ˆä»ä¸‹é¢çš„åˆ—è¡¨ä¸­é€‰æ‹©æˆ–è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹ï¼‰\n",
    "MODEL_NAME = \"zoomerwork/model_gemma\"\n",
    "\n",
    "# é‡åŒ–é€‰é¡¹ï¼ˆå¦‚æœå†…å­˜ä¸è¶³ï¼Œè®¾ç½®ä¸ºTrueï¼‰\n",
    "USE_4BIT = False  # 4bité‡åŒ–ï¼Œå¤§å¹…å‡å°‘å†…å­˜å ç”¨\n",
    "USE_8BIT = False  # 8bité‡åŒ–\n",
    "\n",
    "# è¯„ä¼°é€‰é¡¹\n",
    "SHOW_GENERATION_SAMPLES = True  # æ˜¯å¦æ˜¾ç¤ºç”Ÿæˆæ ·ä¾‹\n",
    "\n",
    "print(\"é…ç½®å®Œæˆï¼\")\n",
    "print(f\"æ¨¡å‹: {MODEL_NAME}\")\n",
    "print(f\"4bité‡åŒ–: {USE_4BIT}\")\n",
    "print(f\"8bité‡åŒ–: {USE_8BIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“‹ æ¨èæ¨¡å‹åˆ—è¡¨\n",
    "\n",
    "**å°å‹æ¨¡å‹** (é€‚åˆCPUæˆ–å°GPUï¼Œ1-3Bå‚æ•°):\n",
    "- `TinyLlama/TinyLlama-1.1B-Chat-v1.0` â­ æ¨èæ–°æ‰‹\n",
    "- `microsoft/phi-2` (2.7B)\n",
    "- `stabilityai/stablelm-2-1_6b` (1.6B)\n",
    "\n",
    "**ä¸­å‹æ¨¡å‹** (éœ€è¦GPUï¼Œ7-13Bå‚æ•°):\n",
    "- `mistralai/Mistral-7B-Instruct-v0.2`\n",
    "- `google/gemma-7b-it`\n",
    "- `meta-llama/Llama-2-7b-chat-hf` (éœ€è¦HF token)\n",
    "\n",
    "**å¤§å‹æ¨¡å‹** (éœ€è¦å¤§æ˜¾å­˜æˆ–é‡åŒ–ï¼Œ30B+):\n",
    "- `meta-llama/Llama-2-70b-chat-hf` (å»ºè®®USE_4BIT=True)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 4: åŠ è½½æ¨¡å‹\n",
    "\n",
    "è¿™ä¸€æ­¥ä¼šä»HuggingFaceä¸‹è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "æ­£åœ¨ä¸‹è½½æ¨¡å‹: zoomerwork/model_gemma\n",
      "ç›®æ ‡è®¾å¤‡: cuda\n",
      "============================================================\n",
      "\n",
      "ğŸ“¥ åŠ è½½Tokenizer...\n",
      "âœ… TokenizeråŠ è½½å®Œæˆ\n",
      "ğŸ“¥ åŠ è½½æ¨¡å‹...\n",
      "âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºè¯„ä¼°å™¨\n",
    "evaluator = LLMEvaluator(MODEL_NAME, device=\"auto\")\n",
    "\n",
    "# ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹\n",
    "evaluator.download_and_load_model(\n",
    "    load_in_4bit=USE_4BIT,\n",
    "    load_in_8bit=USE_8BIT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 5: è¿è¡Œå®Œæ•´è¯„ä¼°\n",
    "\n",
    "è¿™å°†è¿è¡Œæ‰€æœ‰4ä¸ªè¯„ä¼°ç»´åº¦å¹¶ç»™å‡ºç»¼åˆè¯„åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ” å¼€å§‹è¯„ä¼°æ¨¡å‹: zoomerwork/model_gemma\n",
      "======================================================================\n",
      "\n",
      "[1/4] ğŸ“Š è¯„ä¼°å›°æƒ‘åº¦ (Perplexity)...\n",
      "   åŠ è½½WikiText-2æ•°æ®é›†...\n",
      "   å¤„ç†ä¸­: 10/50\n",
      "   å¤„ç†ä¸­: 20/50\n",
      "   å¤„ç†ä¸­: 30/50\n",
      "   å¤„ç†ä¸­: 40/50\n",
      "   å¤„ç†ä¸­: 50/50\n",
      "   âœ… å›°æƒ‘åº¦: 25.26 ğŸ˜Š è‰¯å¥½\n",
      "\n",
      "[2/4] ğŸ“ è¯„ä¼°ç”Ÿæˆè´¨é‡...\n",
      "   ç”Ÿæˆä¸­ 1/5: Explain quantum computing in simple term...\n",
      "   ç”Ÿæˆä¸­ 2/5: Write a short story about a robot:...\n",
      "   ç”Ÿæˆä¸­ 3/5: What is the capital of France?...\n",
      "   ç”Ÿæˆä¸­ 4/5: Translate 'Hello' to Spanish:...\n",
      "   ç”Ÿæˆä¸­ 5/5: List three benefits of exercise:...\n",
      "\n",
      "   âœ… å¹³å‡ç”Ÿæˆæ—¶é—´: 0.58ç§’\n",
      "\n",
      "[3/4] ğŸ“ è¯„ä¼°æ¨¡å‹è§„æ¨¡...\n",
      "   å‚æ•°é‡: 2,614,341,888 (2.61B)\n",
      "   æ¨¡å‹å¤§å°: 4986.46 MB (4.87 GB)\n",
      "\n",
      "[4/4] âš¡ è¯„ä¼°æ¨ç†é€Ÿåº¦ (è¿è¡Œ10æ¬¡)...\n",
      "   âœ… æ¨ç†é€Ÿåº¦: 67.03 tokens/s ğŸš€ ä¼˜ç§€\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»\n",
      "======================================================================\n",
      "å›°æƒ‘åº¦:      25.26\n",
      "æ¨ç†é€Ÿåº¦:    67.03 tokens/s\n",
      "ç”Ÿæˆæ—¶é—´:    0.58s\n",
      "å‚æ•°é‡:      2.61B\n",
      "======================================================================\n",
      "ç»¼åˆè¯„åˆ†:    95.0/100 ğŸ† ä¼˜ç§€\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ ç”Ÿæˆæ ·ä¾‹:\n",
      "\n",
      "[æ ·ä¾‹ 1]\n",
      "Q: Explain quantum computing in simple terms:\n",
      "A: ...\n",
      "\n",
      "[æ ·ä¾‹ 2]\n",
      "Q: Write a short story about a robot:\n",
      "A: The robot, with its metal limbs and circuits, was created with one purpose: to serve. It had no emotions, no thoughts, no dreams. It only knew its tas...\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œå®Œæ•´è¯„ä¼°\n",
    "results = evaluator.run_comprehensive_evaluation(show_samples=SHOW_GENERATION_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 6: ä¿å­˜ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: evaluation_zoomerwork_model_gemma.json\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSON\n",
    "output_filename = f\"evaluation_{MODEL_NAME.replace('/', '_')}.json\"\n",
    "\n",
    "with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"\\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¬ å¯é€‰ï¼šå•ç‹¬æµ‹è¯•å„ä¸ªç»´åº¦\n",
    "\n",
    "å¦‚æœä½ æƒ³å•ç‹¬æµ‹è¯•æŸä¸ªç»´åº¦ï¼Œå¯ä»¥è¿è¡Œä¸‹é¢çš„ä»£ç å—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å•ç‹¬æµ‹è¯•å›°æƒ‘åº¦\n",
    "# custom_texts = [\n",
    "#     \"äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚\",\n",
    "#     \"Machine learning is a subset of artificial intelligence.\",\n",
    "#     \"æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚\"\n",
    "# ]\n",
    "\n",
    "# ppl = evaluator.evaluate_perplexity(texts=custom_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å•ç‹¬æµ‹è¯•ç”Ÿæˆè´¨é‡ï¼ˆä½¿ç”¨è‡ªå®šä¹‰é—®é¢˜ï¼‰\n",
    "# custom_prompts = [\n",
    "#     \"ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼š\",\n",
    "#     \"å†™ä¸€ä¸ªå…³äºæœªæ¥çš„çŸ­æ•…äº‹ï¼š\",\n",
    "#     \"What are the benefits of learning Python?\"\n",
    "# ]\n",
    "\n",
    "# gen_results = evaluator.evaluate_generation_quality(prompts=custom_prompts)\n",
    "\n",
    "# # æ˜¾ç¤ºè¯¦ç»†ç»“æœ\n",
    "# for i, sample in enumerate(gen_results[\"samples\"], 1):\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"é—®é¢˜ {i}: {sample['prompt']}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     print(f\"å›ç­”: {sample['response']}\")\n",
    "#     print(f\"è€—æ—¶: {sample['time']:.2f}ç§’\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å•ç‹¬æµ‹è¯•æ¨ç†é€Ÿåº¦ï¼ˆæ›´å¤šè¿è¡Œæ¬¡æ•°ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœï¼‰\n",
    "# speed_results = evaluator.evaluate_inference_speed(num_runs=20)\n",
    "\n",
    "# print(f\"\\nå¹³å‡æ¨ç†æ—¶é—´: {speed_results['avg_inference_time']:.3f}ç§’\")\n",
    "# print(f\"æ¨ç†é€Ÿåº¦: {speed_results['tokens_per_second']:.2f} tokens/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ æ¯”è¾ƒå¤šä¸ªæ¨¡å‹\n",
    "\n",
    "å¦‚æœä½ æƒ³æ¯”è¾ƒå¤šä¸ªæ¨¡å‹ï¼Œå¯ä»¥è¿è¡Œä¸‹é¢çš„ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # å®šä¹‰è¦æ¯”è¾ƒçš„æ¨¡å‹åˆ—è¡¨\n",
    "# models_to_compare = [\n",
    "#     \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#     # \"microsoft/phi-2\",\n",
    "#     # æ·»åŠ æ›´å¤šæ¨¡å‹...\n",
    "# ]\n",
    "\n",
    "# comparison_results = []\n",
    "\n",
    "# for model_name in models_to_compare:\n",
    "#     print(f\"\\n\\n{'#'*70}\")\n",
    "#     print(f\"æ­£åœ¨è¯„ä¼°: {model_name}\")\n",
    "#     print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "#     try:\n",
    "#         # åˆ›å»ºæ–°çš„è¯„ä¼°å™¨\n",
    "#         eval = LLMEvaluator(model_name, device=\"auto\")\n",
    "#         eval.download_and_load_model(load_in_4bit=True)  # ä½¿ç”¨4bitèŠ‚çœå†…å­˜\n",
    "        \n",
    "#         # è¿è¡Œè¯„ä¼°\n",
    "#         result = eval.run_comprehensive_evaluation(show_samples=False)\n",
    "        \n",
    "#         # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨\n",
    "#         comparison_results.append({\n",
    "#             \"æ¨¡å‹åç§°\": model_name.split('/')[-1],\n",
    "#             \"å®Œæ•´åç§°\": model_name,\n",
    "#             \"ç»¼åˆè¯„åˆ†\": result.get(\"overall_score\", 0),\n",
    "#             \"å›°æƒ‘åº¦\": result.get(\"perplexity\", \"N/A\"),\n",
    "#             \"æ¨ç†é€Ÿåº¦\": result.get(\"inference_speed\", {}).get(\"tokens_per_second\", \"N/A\"),\n",
    "#             \"å‚æ•°é‡(B)\": result.get(\"model_size\", {}).get(\"total_params\", 0) / 1e9,\n",
    "#         })\n",
    "        \n",
    "#         # æ¸…ç†å†…å­˜\n",
    "#         del eval\n",
    "#         torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ è¯„ä¼°å¤±è´¥: {e}\")\n",
    "#         comparison_results.append({\n",
    "#             \"æ¨¡å‹åç§°\": model_name.split('/')[-1],\n",
    "#             \"é”™è¯¯\": str(e)\n",
    "#         })\n",
    "\n",
    "# # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼\n",
    "# df = pd.DataFrame(comparison_results)\n",
    "# df = df.sort_values(\"ç»¼åˆè¯„åˆ†\", ascending=False)\n",
    "\n",
    "# print(\"\\n\\n\" + \"=\"*100)\n",
    "# print(\"ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ\")\n",
    "# print(\"=\"*100)\n",
    "# print(df.to_string(index=False))\n",
    "# print(\"=\"*100)\n",
    "\n",
    "# # ä¿å­˜å¯¹æ¯”ç»“æœ\n",
    "# df.to_csv(\"model_comparison.csv\", index=False, encoding='utf-8-sig')\n",
    "# print(\"\\nğŸ’¾ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ è¯„åˆ†æ ‡å‡†è¯´æ˜\n",
    "\n",
    "### ç»¼åˆè¯„åˆ† (0-100åˆ†)\n",
    "\n",
    "| åˆ†æ•°èŒƒå›´ | ç­‰çº§ | è¯´æ˜ |\n",
    "|---------|------|------|\n",
    "| 90-100 | ğŸ† ä¼˜ç§€ | ç”Ÿäº§çº§åˆ«ï¼Œé€‚åˆå®é™…éƒ¨ç½² |\n",
    "| 75-89 | ğŸ¥ˆ è‰¯å¥½ | é€‚åˆå¤§å¤šæ•°åº”ç”¨åœºæ™¯ |\n",
    "| 60-74 | ğŸ¥‰ ä¸­ç­‰ | ç‰¹å®šåœºæ™¯å¯ç”¨ |\n",
    "| <60 | ğŸ“ éœ€æ”¹è¿› | å®éªŒæ€§è´¨ |\n",
    "\n",
    "### å„ç»´åº¦è¯„åˆ†\n",
    "\n",
    "**å›°æƒ‘åº¦ (30åˆ†)**\n",
    "- < 20: ä¼˜ç§€ (30åˆ†)\n",
    "- 20-50: è‰¯å¥½ (25åˆ†)\n",
    "- 50-100: ä¸­ç­‰ (20åˆ†)\n",
    "- \\> 100: éœ€æ”¹è¿› (10-15åˆ†)\n",
    "\n",
    "**æ¨ç†é€Ÿåº¦ (30åˆ†)**\n",
    "- \\> 50 tokens/s: ä¼˜ç§€ (30åˆ†)\n",
    "- 30-50 tokens/s: è‰¯å¥½ (25åˆ†)\n",
    "- 10-30 tokens/s: ä¸­ç­‰ (15-20åˆ†)\n",
    "- < 10 tokens/s: è¾ƒæ…¢ (10åˆ†)\n",
    "\n",
    "**ç”Ÿæˆè´¨é‡ (25åˆ†)**\n",
    "- åŸºäºå¹³å‡ç”Ÿæˆæ—¶é—´å’Œå“åº”è´¨é‡\n",
    "\n",
    "**æ¨¡å‹æ•ˆç‡ (15åˆ†)**\n",
    "- 1B-10Bå‚æ•°: æœ€ä½³å¹³è¡¡ (15åˆ†)\n",
    "- < 1B: è¾ƒå° (12åˆ†)\n",
    "- 10B-20B: è¾ƒå¤§ (10åˆ†)\n",
    "- \\> 20B: å¾ˆå¤§ (5åˆ†)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ä½¿ç”¨å»ºè®®\n",
    "\n",
    "1. **é¦–æ¬¡è¿è¡Œ**: å»ºè®®ä»å°æ¨¡å‹å¼€å§‹ï¼ˆå¦‚TinyLlamaï¼‰\n",
    "2. **å†…å­˜ä¸è¶³**: å¯ç”¨4bité‡åŒ– (`USE_4BIT=True`)\n",
    "3. **GPUåŠ é€Ÿ**: ç¡®ä¿å®‰è£…äº†CUDAç‰ˆæœ¬çš„PyTorch\n",
    "4. **ä¸‹è½½åŠ é€Ÿ**: å¯ä»¥è®¾ç½®HuggingFaceé•œåƒ\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## â“ å¸¸è§é—®é¢˜\n",
    "\n",
    "**Q: ä¸‹è½½é€Ÿåº¦æ…¢æ€ä¹ˆåŠï¼Ÿ**\n",
    "- A: è®¾ç½®HuggingFaceé•œåƒæˆ–ä½¿ç”¨ä»£ç†\n",
    "\n",
    "**Q: å†…å­˜æº¢å‡º (OOM) æ€ä¹ˆåŠï¼Ÿ**\n",
    "- A: å¯ç”¨4bité‡åŒ–ï¼Œæˆ–é€‰æ‹©æ›´å°çš„æ¨¡å‹\n",
    "\n",
    "**Q: å¦‚ä½•è¯„ä¼°ç§æœ‰æ¨¡å‹ï¼Ÿ**\n",
    "- A: ç¡®ä¿å·²ç™»å½•HuggingFace: `huggingface-cli login`\n",
    "\n",
    "**Q: è¯„ä¼°ç»“æœå¦‚ä½•è§£è¯»ï¼Ÿ**\n",
    "- A: å‚è€ƒä¸Šé¢çš„è¯„åˆ†æ ‡å‡†ï¼Œç»¼åˆè€ƒè™‘å„é¡¹æŒ‡æ ‡\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ License\n",
    "\n",
    "MIT License - å¯è‡ªç”±ä½¿ç”¨å’Œä¿®æ”¹\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "unsloth_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
